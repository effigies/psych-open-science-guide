Reproducible data analysis
==========================

Authors: Shashank Bansal, Russ Poldrack, Jon Walters, Alexandre Cionca

The primary goal of reproducible data analysis is to ensure computational reproducibility --- that is, the ability of another researcher to use one’s code and data to independently obtain identical results. However, the fact that a particular analysis is reproducible does not mean that it is correct.  This is related to the oft-noted distinction in statistics between reliability (which refers to the consistency or precision of the results) and validity (which refers to the accuracy of the results).  Thus, a second goal of reproducible data analysis is to ensure that the results generated by the code are correct. An important aspect of this is to validate the code using software development practices that prevent errors and software testing methods that can help detect them when they occur.

An additional goal is to allow tracking of the provenance of any particular result (figure, table, or other result), which refers to the process through which that result was obtained.  One way to think of this is to imagine the path from a particular result described in a manuscript back to the original data, describing all of the code as well as any intermediate results that are used in generating the published result.  If you have ever struggled to reconstruct exactly how a figure was generated after publishing a paper, then you understand the importance and challenge of provenance tracking.

Committing to reproducibility
-----------------------------

There are a number of coding practices that will help enhance reproducibility. While it takes a bit of time to learn and implement these practices, they will pay off in the long run. As the Sofware Carpentry authors say:

The good news is, doing these things will speed up our programming, not slow it down. As in real carpentry — the kind done with lumber — the time saved by measuring carefully before cutting a piece of wood is much greater than the time that measuring takes. [https://swcarpentry.github.io/python-novice-inflammation/10-defensive/index.html](https://www.google.com/url?q=https://swcarpentry.github.io/python-novice-inflammation/10-defensive/index.html&sa=D&ust=1596473422932000&usg=AOvVaw2axw38UL82HSfcsmIHXXQf)

There are several global decisions that one can make that will make it easier to work in a reproducible way.  These are admittedly opinionated views that may be differently applicable in different research domains.

-   Use only free/open-source software whenever possible. This makes it easier for anyone else to reproduce your work without needing to buy particular software.

&nbsp;

-   In some fields, commercial software platforms are standard.  When free/open source alternatives are available (e.g. Octave for MATLAB), try to ensure that code is compatible with those alternatives whenever possible.

&nbsp;

-   Minimize manual analysis steps.  Any analysis operation that can be automated should be automated, using some form of script.  Never perform manual data reorganization or file renaming operations.
-   Commit to a standard organization scheme.  Using a standard scheme may sometimes require a bit more work in the short term, but will have significant payoffs in the longer term.
-   Commit to improving your skills as a software developer.  Software development is a set of skills that must be learned, just like a new spoken language or musical instrument.  The best way to advance one’s skills is through consistent and deliberate practice.  Commit to reading the materials on software engineering that are outlined in the Resource section below, and working to implement those techniques in your coding practices.

&nbsp;

-   It can be particularly useful to learn about “anti-patterns” (i.e. commonly used worst practices) for one’s language

&nbsp;

-   Python: [https://docs.quantifiedcode.com/python-anti-patterns/](https://www.google.com/url?q=https://docs.quantifiedcode.com/python-anti-patterns/&sa=D&ust=1596473422933000&usg=AOvVaw16bmucSjPSfBJ0gSePd9D8)
-   R: [The R Inferno](https://www.google.com/url?q=https://www.burns-stat.com/pages/Tutor/R_inferno.pdf&sa=D&ust=1596473422933000&usg=AOvVaw0e59DtIXgcOgEgMSET4SZr)

Prerequisites
=============

In order to get started with reproducible data analysis, you will need to understand several topics:

-   Version control (described in the section on Code Sharing)
-   Using the command line (described in the section on Basic Skills)

Getting started
===============

Step 1: Create a reproducible environment^([\[a\]](#cmnt1)[\[b\]](#cmnt2))
--------------------------------------------------------------------------

The environment comprises all of the software components that are necessary to perform a particular operation.  This includes the code and data as well as any dependencies (such as software libraries) that are necessary to run the code.  

There are two levels of reproducibility that one might shoot for in their software environment.  First is the ability to reproduce the environment on one’s own system.  This can be achieved by generating a virtual environment, which is a configuration of one’s system that can be loaded or unloaded as needed. For both Python, one can use the [Anaconda](https://www.google.com/url?q=https://www.anaconda.com/products/individual&sa=D&ust=1596473422934000&usg=AOvVaw1GCFjp2DSrTcSy6BsRjtwO) software distribution to create and manage virtual environments; one can also use Anaconda to install and manage virtual environments for R, but this can cause problems for RStudio users.  For Rstudio users, a better solution is the[ ](https://www.google.com/url?q=https://rviews.rstudio.com/2019/04/22/reproducible-environments/&sa=D&ust=1596473422935000&usg=AOvVaw0fKnvYBXti-JTFD9M7XHGr)[renv](https://www.google.com/url?q=https://rviews.rstudio.com/2019/04/22/reproducible-environments/&sa=D&ust=1596473422935000&usg=AOvVaw0fKnvYBXti-JTFD9M7XHGr)[ package,](https://www.google.com/url?q=https://rviews.rstudio.com/2019/04/22/reproducible-environments/&sa=D&ust=1596473422935000&usg=AOvVaw0fKnvYBXti-JTFD9M7XHGr) which allows one to snapshot and restore packages.

The second level of reproducibility is to allow someone else to implement the same environment on a different computer.  There are two ways to potentially address this.

1.  One can simply record all of the dependencies that are installed on their system:

-   Anaconda: conda list prints all installed packages and versions.
-   Python: pip freeze prints all installed packages and versions.
-   R: Use [installed.packages](https://www.google.com/url?q=https://www.r-bloggers.com/list-of-user-installed-r-packages-and-their-versions/&sa=D&ust=1596473422936000&usg=AOvVaw0fvgPOzCUIwOvHTj_ttPgH), devtools::package_info or [renv](https://www.google.com/url?q=https://rviews.rstudio.com/2019/04/22/reproducible-environments/&sa=D&ust=1596473422936000&usg=AOvVaw2r0oI5Q7yoKjy6ahAAYkKg) to create a snapshot of current package versions

This will allow someone else with exactly the same operating system to reproduce the development environment, but will not ensure that operating system libraries are the same across the systems.  This is important because there is [evidence](https://www.google.com/url?q=https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/25964757/&sa=D&ust=1596473422937000&usg=AOvVaw20NZjpmZwny9qjYK5hMydV) that analytic results can vary across operating systems.

1.  A generally more effective way to allow someone else to reproduce one’s environment is to use containers.  A container is a system that emulates a virtual computer within one’s own computer, which is fully configurable and allows one to almost exactly reproduce an environment across different computers.  The most commonly used system for containerization is Docker; a related system called Singularity is used on shared computer systems such as clusters.  See the FAQ below for more on how to set up and use Docker and Singularity.

Step 2: Use version control for everything
------------------------------------------

-   All work done on a computer for a project should be tracked using a version control system, including file name changes and reorganization.  This allows the history of all code and operations to be tracked.
-   Develop a strategy for committing your changes, and stick to it.
-   One good strategy from [this blog post](https://www.google.com/url?q=https://jasonmccreary.me/articles/when-to-make-git-commit/&sa=D&ust=1596473422938000&usg=AOvVaw2PBOz8F8QybOtakmGvO8El) to commit:

&nbsp;

-   When you complete a unit of work.
-   When you have changes you may want to undo.

Step 3: Code Understandably
---------------------------

Your code should be understandable to a reader who is familiar with the language, preferably without the need for many comments in the code.  This isn’t just for other people -- it’s also for you when you look back at your code in the future.

-   Practice literate programming whenever possible. Literate programming is an approach to coding in which the logic of the program is explained in natural language alongside the code.  See the section on Reproducible Manuscripts for more on this.
-   Use the [Pseudocode Programming Procedure](https://www.google.com/url?q=https://davidzych.com/writing-code-using-the-pseudocode-programming-process/%23:~:text%3DThe%2520Pseudocode%2520Programming%2520Process%2520defines,This%2520eliminates%2520most%2520commenting%2520effort.&sa=D&ust=1596473422939000&usg=AOvVaw3Ep6chH6xv1lBx_pHdiDjK) to plan your code. Before you start writing code for a project, first plan out the structure of the code using natural language descriptions of its structure (such as the various functions and classes).  The pseudocode can then be retained as comments, or as a first step towards a literate program.
-   Use understandable variable names.  
-   Don’t embed “magic numbers” in your code.  Inserting numbers into the code can cause problems if those values need to change in the future.  Always define clearly named variables to contain any specific value.
-   Use comments sparingly.  

&nbsp;

-   See the article [Clean Code: Comments and Formatting](https://www.google.com/url?q=https://www.todaysoftmag.com/article/1120/clean-code-comments-and-formatting&sa=D&ust=1596473422939000&usg=AOvVaw0eOsqsMGdC-SXLDHc6eHnE) for an overview of Robert Martin’s suggestions on good and bad uses of comments. 

&nbsp;

-   Document functions and classes. Describe all arguments and return values.

&nbsp;

-   Add [doctests (Python) or equivalent](https://www.google.com/url?q=https://docs.python.org/3/library/doctest.html&sa=D&ust=1596473422940000&usg=AOvVaw1jZAxlVPt6EgbHDminvNMP) when possible, which allow the embedding of runnable examples in the documentation, which can then be used as tests

&nbsp;

-   Follow language-specific code conventions when available. This will make it much easier for readers to parse your code.

&nbsp;

-   Python: [https://realpython.com/python-pep8/](https://www.google.com/url?q=https://realpython.com/python-pep8/&sa=D&ust=1596473422940000&usg=AOvVaw1bAbT8SgQUDJ_yJp8yMdgs)
-   R: [Hadley Wickham’s R style guide](https://www.google.com/url?q=http://adv-r.had.co.nz/Style.html&sa=D&ust=1596473422940000&usg=AOvVaw0PpZCc1zZ3fIc5ArhJmcYk), [Google R style guide](https://www.google.com/url?q=https://google.github.io/styleguide/Rguide.html&sa=D&ust=1596473422941000&usg=AOvVaw16cxZimbH76p1BY56UvESF)
-   Javascript: [Google’s javascript style guide](https://www.google.com/url?q=https://google.github.io/styleguide/jsguide.html&sa=D&ust=1596473422941000&usg=AOvVaw0ZbB6MKC3RX2KdTUi4dKvt)

&nbsp;

-   Use a code analyzer.  If available, a code analyzer can identify errors or potential problems in one’s code, in addition to identifying compliance with style conventions.

&nbsp;

-   Python: [flake8](https://www.google.com/url?q=https://flake8.pycqa.org/en/latest/&sa=D&ust=1596473422941000&usg=AOvVaw3vqUMbf4ERmU8miOjrfURL)
-   R: [lintr](https://www.google.com/url?q=https://cran.r-project.org/web/packages/lintr/readme/README.html&sa=D&ust=1596473422942000&usg=AOvVaw2-Dm2zekpcYPo_telfiMrN)

Step 4: Code defensively
------------------------

Assume that errors will occur, and create code that will be robust to those errors and/or will call attention to them when they exist.

-   Use assertions. Assertions are statements that will cause a program to signal an error if a particular condition is false. These should be used to detect problematic conditions.

&nbsp;

-   Example: If a number is meant to be a probability, then its value must be within the range \[0, 1\]:

&nbsp;

-   Python: assert p \>= 0 and p \<= 1

&nbsp;

-   Write tests for important functions.  Tests can ensure that a function performs properly, and can test for its ability to handle potential errors.

&nbsp;

-   For data analysis functions, it can be useful to generate dummy data and assert that your function returns the expected outcome when applied to the dummy data, and that it also loudly fails when given inappropriate inputs
-   See more in the FAQs below below on Software Testing

&nbsp;

-   Be aware of random seeds.  In any code that uses random number generators (RNG), the results may vary depending upon the initialization of the RNG.  By default, the random seed is usually set based on the current time when the RNG is first used in a session, which will give a different set of random numbers each time the program is run.  

&nbsp;

-   One can explicitly set the random seed in order to ensure that the same random numbers are used each time the program is run.

&nbsp;

-   In some cases this is useful, but it could also be problematic, as it limits the generalizability of the results to that particular random seed.

&nbsp;

-   A good solution is to obtain a random number using the default (time-based) seed, use this number as the random seed but also store it for later reference. This would allow a user in the future to re-run the program using exactly the same seed, which should allow exact reproducibility even with random numbers..  

Step 5: Code portably
---------------------

Assume that your code will need to run on other computers, so that details about your specific computer should not be stored in the code itself.

-   Never use absolute paths such as /Users/username/code/projectname

&nbsp;

-   Instead, specify a base path that can be modified in one place, and use that base path to create all other paths.

&nbsp;

-   Never store credentials or secrets in code.  

&nbsp;

-   Cybercriminals regularly check github for credentials (such as access keys for cloud computing systems like AWS), and may use those keys to commit crimes
-   In addition, these secrets may need to change for portability across different systems.

&nbsp;

-   Use configuration files or environment variables

&nbsp;

-   Instead of storing details about the system in code, store them in a configuration file or an environment variable.
-   Be sure not to check in the configuration file to your git repository!

&nbsp;

-   Add the name of the configuration file to your [.gitignore file](https://www.google.com/url?q=https://www.atlassian.com/git/tutorials/saving-changes/gitignore&sa=D&ust=1596473422944000&usg=AOvVaw0o-yxBEqF92GX_BWw8IDuC), which will prevent it from being seen by git.

Step 6: Automate your workflow
------------------------------

-   The goal of automation is to allow one to issue a single command that can execute the entire workflow.
-   Workflow automation brings many benefits.  

&nbsp;

-   It allows one to easily rerun an entire workflow if there are changes to the input data or preprocessing.
-   It saves the researcher time, because they don’t have to recreate a complex set of steps to run an analysis.
-   It ensures that the entire path of the analysis, from raw data to final results, is documented and the provenance of the final results are clear.
-   It makes it much easier to implement analysis approaches (such as multiverse analyses) that require running the workflow many times with different parameters, or resampling methods that rerun the workflow repeatedly on subsamples of the data.

&nbsp;

-   There are many different tools for workflow automation.

&nbsp;

-   Some are domain-specific

&nbsp;

-   Neuroimaging: [nipype](https://www.google.com/url?q=https://nipype.readthedocs.io/en/latest/&sa=D&ust=1596473422945000&usg=AOvVaw1NC6Fb2vFQWuUBPzhgQsMA)
-   Computational biology: [Galaxy](https://www.google.com/url?q=https://galaxyproject.org/&sa=D&ust=1596473422946000&usg=AOvVaw15N534xQ1UCTeL8exR34wZ)

&nbsp;

-   A good general-purpose tool for workflow automation is UNIX make

&nbsp;

-   See the FAQs below for more on how to automate a workflow using Makefiles

Advanced steps
==============

Step 7: Containerize your workflow using Docker
-----------------------------------------------

Docker allows one to generate a system with a single environment that can be reused by anyone, which contains specific versions all the dependencies that an analysis, or other code, needs to run.  In essence, Docker creates a virtual Linux-based computer running inside your computer.

1.   Install the [Docker Desktop](https://www.google.com/url?q=https://www.docker.com/products/docker-desktop&sa=D&ust=1596473422946000&usg=AOvVaw0Itos6MDXbc8kN49A6keuu) software for Mac and most recent Windows

&nbsp;

1.  If you are working on a shared system such as a cluster, you will still need to use Docker on your local machine to generate the container, since creating a Docker container requires administrative privileges which are not available on a shared system.

&nbsp;

1.  If you are not familiar with Docker, complete the first three sections of [A Docker Tutorial for Beginners](https://www.google.com/url?q=https://docker-curriculum.com/&sa=D&ust=1596473422947000&usg=AOvVaw2pfYEzIMbunV07ROYcaJ_Q) (through the “Hello World” section)

&nbsp;

1.  For more information, also see the video [How to get started with Docker](https://www.google.com/url?q=https://youtu.be/iqqDU2crIEQ?t%3D30&sa=D&ust=1596473422947000&usg=AOvVaw3B0bnT0qb_c3nM0gMGdmQf) and the [Docker Quickstart](https://www.google.com/url?q=https://docs.docker.com/get-started/&sa=D&ust=1596473422947000&usg=AOvVaw0TXnGFoxm_3R27nP2yRtnS) guide

&nbsp;

1.  Create a Dockerfile that contains all of your necessary dependencies

&nbsp;

1.  This often requires trial and error, as it may not be immediately obvious which libraries are necessary and/or how to find them for Linux

&nbsp;

1.  Note: creating a Dockerfile requires one to understand how to install packages on a Linux system.  If you are not comfortable with installing packages from the command line using apt-get, [here is an introduction](https://www.google.com/url?q=https://itsfoss.com/apt-get-linux-guide/&sa=D&ust=1596473422948000&usg=AOvVaw1KSKCPZuoaFp0aOKP9WEwv).

&nbsp;

1.  TBD: find or create a good tutorial on creating a dockerfile, using the ubuntu base image

&nbsp;

1.  Be sure to specify versions of all packages, as described below.

&nbsp;

1.  Build your container

&nbsp;

1.  It can be useful to set up a Makefile containing your docker commands so that you can build the container using a simple command like make docker-build

&nbsp;

1.  Run your analysis within a Docker container
2.  Push your container to [DockerHub](https://www.google.com/url?q=https://hub.docker.com/&sa=D&ust=1596473422949000&usg=AOvVaw2nCuoFmPKyLzsjzw1TPbom)

&nbsp;

1.  This will allow others to use your container without the need to build it themselves. It also allows you to refer to a specific fixed version of the container, so that others can use exactly the same version

Step 8: Run your workflow automatically using continuous integration
--------------------------------------------------------------------

-   Continuous integration systems provide the ability to automatically run a particular code whenever a new commit is pushed to a Github repository.
-   This is primarily meant to be used for software testing, but can also be leveraged to actually run scientific data analyses in a fully reproducible way.

&nbsp;

-   The results of the analysis can be exposed as “artifacts” and uploaded to a data repository to be shared.

&nbsp;

-   Examples:

&nbsp;

-   [NARPS](https://www.google.com/url?q=https://github.com/poldrack/narps&sa=D&ust=1596473422949000&usg=AOvVaw1mK-pwkenzWOZQiz55NbxP)

 

Frequently Asked Questions
==========================

### How do I set up a Python environment:

-   Install the latest version of the Anaconda package^([\[c\]](#cmnt3)[\[d\]](#cmnt4)): [https://www.anaconda.com/distribution/](https://www.google.com/url?q=https://www.anaconda.com/distribution/&sa=D&ust=1596473422950000&usg=AOvVaw177TFvOfafsRSySRIJClgz)

&nbsp;

-   Be sure to download the Python 3.X version, since Python 2.7 is being phased out.

### How do I set up an R/RStudio environment?

-   Install latest version of R from one of the R mirrors, such as [http://lib.stat.cmu.edu/R/CRAN/](https://www.google.com/url?q=http://lib.stat.cmu.edu/R/CRAN/&sa=D&ust=1596473422951000&usg=AOvVaw2xg_m6uuQQo0ZHbJjxOPE5)
-   Install latest version of RStudio from [https://rstudio.com/products/rstudio/download/](https://www.google.com/url?q=https://rstudio.com/products/rstudio/download/&sa=D&ust=1596473422951000&usg=AOvVaw0idABuYVul51QXFLB3J42I)
-   Alternatively, if you are going to be using Python and R together, [install R using Anaconda](https://www.google.com/url?q=https://www.guru99.com/download-install-r-rstudio.html&sa=D&ust=1596473422952000&usg=AOvVaw0lsDPzSOiDzG_oZSOJjyM4) (after installing Anaconda as listed below)

&nbsp;

-   Note that if you use R via anaconda, it’s important to install packages via conda rather than via R.  See [here](https://www.google.com/url?q=https://community.rstudio.com/t/why-not-r-via-conda/9438&sa=D&ust=1596473422952000&usg=AOvVaw1m10zdQ1S70WBk28r-wQBy) for further discussion.

### How do I create a Virtual Environment?

-   Within Anaconda you can create virtual environments for Python and R, in which you install a particular set of dependencies.  This can be useful if you have different projects in which you may need to install different software versions.
-   To create a virtual environment with a particular python version:

&nbsp;

-   conda create -n myprojectenv python=3.8

&nbsp;

-   To activate the environment:

&nbsp;

-   conda activate myprojectenv

### How can I install a specific set of package versions in R?

-   In R it is difficult to  specify the package version being used, especially for package dependencies..

&nbsp;

-   One way to achieve exact version reproducibility is to use the Checkpoint package for R ([https://cran.r-project.org/web/packages/checkpoint/index.html](https://www.google.com/url?q=https://cran.r-project.org/web/packages/checkpoint/index.html&sa=D&ust=1596473422953000&usg=AOvVaw08C1sWr8DcFXzXwByo9MXG)).  This package allows you to specify a particular date, and then uses all of the package versions that are current as of that date:

library(checkpoint)

checkpointDir \<- '/checkpoint'

checkpoint("2019-08-13", checkpointLocation = checkpointDir)

-   It is also important to record the package versions that are used so that they can be reported and/or shared with the results upon publication (now required by some publishers such as Nature).  You can use the package_info() function from devtools to get this information and save it to a text file:

library(devtools)

package_info \<- devtools::package_info()

write.table(package_info, file=paste(‘R_package_info.txt)

### How can I install a specific version of a Python package?

-   You can install specific versions of python packages using:

&nbsp;

-   pip (e.g. pip install MySQL_python==1.2.2)
-   conda (e.g. conda install scipy=0.15.0).  

### What is Singularity?

[Singularity](https://www.google.com/url?q=https://singularity.lbl.gov/&sa=D&ust=1596473422955000&usg=AOvVaw2h85M6j18jtIK0_GI7ECNR) provides the ability to use containers like Docker without requiring adminstrative access, so that it’s usable with high performance computing clusters (HPCs). Recent versions of singularity can use images directly from DockerHub, making it very easy to implement once an image has been pushed to DockerHub.

### How should I structure my repository?

-   See the section on Code Sharing for more on how to structure a repository

    ------------------------------------------------------------------------

#### Best practice for analysis execution

Before running an analysis script, you should get familiar with its different mechanisms and options. Listing and explaining the decisions that you made will help you keep track of your work and help others find their optimal execution procedure. Here are some steps to guide you in this practice:

1.  Understand the main process and options of the code

&nbsp;

1.  List types of execution and differences
2.  Propose an explanation of the procedure
3.  Discuss your choice and expectations
4.  Come up with justifications linked with your hypothesis

1.  Describe parameters

&nbsp;

1.  List parameters along with their influence on the results
2.  Discuss values, alternative, compromise and, if possible, propose justifications for all of them.

        

Since your analysis script could probably be successfully executed from the command line, a simple, yet robust and more general, method would be to run it from a bash script. The use of bash allows to perform tasks such as files and parameters management and testing in a replicable and efficient manner through different operating systems. Eventually, the goal is to help people run your analysis with the fewer efforts and understanding of the in-depth details of your code.

As an example, your bash script could include some conditions on the type or the name of the data that will influence the parameters or type of execution of your analysis. Furthermore, these tests may also provide warnings and printed errors if something is wrong or not compatible.

Be aware of numerical precision issues.  When using floating point arithmetic functions, different systems may give different answers due to rounding issues related to numerical precision and rounding.

-   See [here](https://www.google.com/url?q=https://stackoverflow.com/questions/21212326/floating-point-arithmetic-and-reproducibility&sa=D&ust=1596473422957000&usg=AOvVaw33-hsgjq4GpeQpTbLS_phF) for discussion of this topic.

Resources
=========

Provenance

-   [Provenance tracking](https://www.google.com/url?q=https://rrcns.readthedocs.io/en/latest/provenance_tracking.html&sa=D&ust=1596473422958000&usg=AOvVaw2kEZ0x5dGFBzd4FIOfms84)

-   Reading^([\[e\]](#cmnt5)) resources:

&nbsp;

-   [The Turing Way: Reproducible Environments](https://www.google.com/url?q=https://the-turing-way.com/reproducible_environments/reproducible_environments.html&sa=D&ust=1596473422959000&usg=AOvVaw23EihL5Pe92XmXN4qaevrz)

&nbsp;

-   Tools:

&nbsp;

-   [NeuroDocker](https://www.google.com/url?q=https://github.com/ReproNim/neurodocker&sa=D&ust=1596473422959000&usg=AOvVaw0XcWncZZwUYdCi3CZqF8Sj) simplifies the creation of Docker/Singularity recipes

[https://www.ihrp.uic.edu/files/Workflow%20Slides%20JSLong%20110410.pdf](https://www.google.com/url?q=https://www.ihrp.uic.edu/files/Workflow%2520Slides%2520JSLong%2520110410.pdf&sa=D&ust=1596473422959000&usg=AOvVaw2KuwMoO9Z5TPPOKOrrvl5r)

[\[a\]](#cmnt_ref1)creating a requirements.txt for tracking environment dependencies?

[\[b\]](#cmnt_ref2)+1 ... similarly, I think my question here is, okay is this at the level of a specific r markdown file or at a higher level?

[\[c\]](#cmnt_ref3)please add a point/reference on how to install R using anaconda e.g. the command

[\[d\]](#cmnt_ref4)+1

[\[e\]](#cmnt_ref5)Are there any other steps other than recording dependencies?

Like, I'm thinking don't put absolute path name ...
